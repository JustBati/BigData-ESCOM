{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 PySpark\n",
    "- Context es la version antigua y original para la inicializacion de sesiones de PySpark\n",
    "- Spark esta basado en Java\n",
    "- ***Al utilizar Spark manejar un debuggeo constante dado a su naturaleza flexible el cual arroja errores on runtime***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea la unidad de trabajo de manera local, y se le asigna nombre al area de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = SparkContext(\"local\",\"PRUEBA1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cereamos una variable utilizando la variable de *SparkContext* para paralelizar un vector\n",
    "- Creando asi un objeto de tipo *rdd*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeros = [1,2,3,4,5]\n",
    "rdd1 = contexto.parallelize(numeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuando se llegue a utilizar un metodo, se tiene que utilizar el metodo *map* que se aplica mediante funciones *lamda*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddcuadrados = rdd1.map(lambda Vector_x:Vector_x*Vector_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Para poder imprimir el contenido de un *rdd* se utiliza el metodo *.colect()* para acceder al contenido del objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rddcuadrados.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con el metodo *.zip()* se pueden agrupar vectores de informacion en un mismo rdd o vectores, creando de esta manera un *vector de vectores*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddsuma = rdd1.zip(rddcuadrados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Al utilizar el anidamento de esta manera nos permite utilizar las funciones lamba nos que solo aceptan de un vector a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddsuma = rdd1.zip(rddcuadrados).map(lambda vector: vector[0] + vector[1])\n",
    "print(rddsuma.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es importante destacar que al terminar un trabajo de PySpark se cierre la sesion de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
