{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 101\n",
    "- Spark Session, nos permite controlaro los datos de forma similar a Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- De igual manera que con *Context* tenemos que inicializar la sesion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesion = SparkSession.builder.appName(\"Leer CSV\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utiliazando el objeto de la sesion, leemos el CSV\n",
    "- Accedemos al dataset mediante el path del mismo donde esta alojado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./dataset.csv\"\n",
    "df = sesion.read.csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- De igual manera podemos desplegar el contenido mediante *.show*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Al momento de cargar el CSV se tiene que indicar el header y el esquema del datset\n",
    "- *header=True* indica que se tiene header en las columnas\n",
    "- *inferSchema=True* detecta el esquema de separacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sesion.read.csv(path,header=True,inferSchema=True)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En pyspark no se puede trabajar de manera directa con dataframes ya que estos son considerados un vector de N filas a nivel codigo\n",
    "- Para poder trabajar con los datos de un dataset, ***se tiene que convertir cada columna en un RDD aislando solo los datos requeridos***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columna in df.columns:\n",
    "    print(\"Column data\")\n",
    "    rdd_column = df.select(columna).rdd # Se crea un vector de matriz de la columna necesitada\n",
    "    rdd_values = df.select(columna).rdd.map(lambda row: row[0]) # De esta manera aislamos el dato de la columna que requerimos, arrojando un vector de valores\n",
    "    print(rdd_values.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
