{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 PySpark\n",
    "- Context es la version antigua y original para la inicializacion de sesiones de PySpark\n",
    "- Spark esta basado en Java\n",
    "- ***Al utilizar Spark manejar un debuggeo constante dado a su naturaleza flexible el cual arroja errores on runtime***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se crea la unidad de trabajo de manera local, y se le asigna nombre al area de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = SparkContext(\"local\",\"PRUEBA1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cereamos una variable utilizando la variable de *SparkContext* para paralelizar un vector\n",
    "- Creando asi un objeto de tipo *rdd*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeros = [1,2,3,4,5]\n",
    "rdd1 = contexto.parallelize(numeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El metodo *.reduce* esta a la par del metodo *.map* simplemente que el metodo *.map* es para trabajar con vectores y en el sig ejemplo trabajaremos para sacar un valor de una operacion de un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = rdd1.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El metodo *.zipWhithIndex* arroja el vector o rdd inicial en tuplas con el que seria su index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_index = rdd1.zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En el sig codigo vamos a realizar las siguientes acciones\n",
    "    1. Utilizando el vector con indice en tuplas vamos a filtrar\n",
    "    2. Donde el fintro va a ser que el primer elemento (indice) sea igual a un valor\n",
    "    3. Esto nos regresa un vector con una tupla, por ende si queremos acceder a los valores del mismo necesitamos utilizar el metodo *.collect*\n",
    "    4. Al acceder al valor del vector nos da la tupla, por lo que accedemos al valor que requerimos de la tupla mediante sintaxis basica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_index = rdd_index.filter(lambda x: x[0]==3).collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
